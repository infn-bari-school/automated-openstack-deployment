HTCondor, exploit batch resource usage through a community driven Local Resource Management System
Overview
The ReCaS DataCenter will face a big challenge since it will start to operate because it has to provide computing resources to a wide range of scientific communities and end users who will exploit it through the batch job submission. How the resources will be allocated is a key consideration in the resource management process because the user committed resources must be granted and at the same time we can’t afford to waste a single cpu cycle. Achieving these results at ReCaS DataCenter scale is even more complex because every single aspect has a considerable size like for instance the 15K cores available. We have a must, the resource management process must be stable and reliable in order to efficiently allocate the available resources. Last but not least  the computing resources hosted at ReCaS datacenter must be accessible through different interfaces. In particular the GRID interface will enable end users spread around the world to access the resources in a transparent way.
HTCondor is emerging as a valid tool to manage computing resources in a stable, efficient and smart manner. The main purpose of this document is to try to demonstrate  the sustainability of this tool as Local Resource Management System for ReCaS DataCenter both from technical point of  view and the way the choice of this tool will affect the ReCaS annual budget. To do so we have deployed a virtual condor pool of the same size of that one which will deployed in production in order to simulate real use cases. What we have seen and learned could be  really amazing.
2.HTCondor overview
For the purposes of understanding this paper, a brief overview of the HTCondor architecture is helpful. A HTCondor pool is defined by a collector, which serves as a registry for the rest of the distributed daemons in the pool. Each daemon is described by a ClassAd record, which the daemon periodically sends to the collector to advertise its presence and current status in the pool. Job execution nodes in the pool are represented by a startd, which is responsible for carrying out job execution requests. The startd divides the machine into one or more logical subdivisions called execution slots. A collection of jobs that have been submitted by users is maintained by a schedd. The schedd obtains a lease to run jobs on an execution slot. The lease is obtained from the negotiator, which is the daemon responsible for pool-wide user priority management and for matchmaking. Matchmaking involves finding compatible execution slots for the resource requests (in the form of ClassAds) the schedd makes on behalf of the users.
3.1HTCondor  architecture sustainability 
The HTCondor architecture is highly distributed. Actually you can execute each HTCondor component on a dedicated host so this means you don’t have a monolithic service dealing with the entire cluster but you can distribute load and functionalities among pool components and of course this a good starting point. 
It is possible to have multiple collector instances because it can just be considered as a keeper of the cluster status in terms of active clients ClassAds. Even better it is strong suggested to have more than one collector running at the same time because it is a mandatory requirement in order to deploy the negotiator in high availability. As well as the collector you can run multiple schedd instances which do not share the entire collection of batch jobs that have been submitted to the pool. Actually each schedd is in a charge to handle the batch jobs which have been directly submitted to  it, as matter of fact it is completely unaware about batch jobs submitted to another schedd. Pay attention that this feature is not intended to let the pool to scale because each schedd instance can successfully manage more than one hundred thousands of batch jobs. 
From architecture point of view HTCondor can be really considered a sustainable choice because there are no bottlenecks as further of matter it can easily sustains a resources incremental in the future.

3.2  HTCondor deployment testbed
Of course we have to looking for a lot of evidences about the declared HTCondor goodness before definitely chose it as LRMS. To do so we have exploited the cloud computing feature to over commit the available resources in order to deploy a HTCondor pool of the same size of the real one. Actually we have deployed an OpenStack cloud cluster in order to simulate a HTCondor pool with nearly 17K slots and 0,7K hosts. We have had the possibility to dynamically instantiate computing resources that automatically joined the HTCondor pool. In this way we have really simulated real use cases such as activating a brand new bunch a computing resources or destroying a bunch of virtual machines simulating for instance a power cut on a rack power line. We have had the possibility to measure how the pool reacts to these circumstances and in the following paragraphs we will describe the results
3.3.1 HTCondor reactiveness
The cluster/pool status in terms of available resource is subject to change over the time. For instance at t0 you can have 100 cores available because you have  five servers with twenty cores each. At t1 the pool status change because one of the server available becomes unavailable so the total core available drops to 80. At The ReCaS DataCenter size the status changes will be a constant  because you have to manage a lot of commodity hardware which is subject to fail very frequently. On the other hand once the broken hardware has been repaired it will join the pool again in production. Sometimes there will be even wide variation due, for instance, to a power cut on a rack power line or in the event to deploy a bunch of new resources. We assume that the state of the cluster will suffer variation over time. The way a LRMS reacts to a status change is an important factor which influences the LRMS main features such as the scheduling decisions which will be based upon cluster/pool status.
Fig. 1: condor_status command reports information about the hosts which have joined the HTCondor pool

Response time to a basic HTCondor command provides a realistic measure of the time needed to access the pool status information so that high value in response time can be considered as an issue which can lead to poor cluster/pool reliability
Fig. 1 shows the condor_status command response time according to  both small or wide variation in the pool status in terms of cores available. According to the measurements the time needed to access the pool status  grows linearly with the amount of data available. Anyway it is important to highlight two remarkable aspects. The first one shows that a wide and sudden variation in the number of cores available generates a significant  increment in the response time for a short period of time. Such condition can be considered expected because the pool is overload and it takes a lot of time to responds, however it does not become unresponsive which is the remarkable aspect. The second one shows   as a slight variation  in the pool status can generate some spikes in terms of response time as the amount of that data representing the pool status increase. Anyway during these stress conditions the HTCondor pool status has been always available.
3.3.2 HTCondor stability
We have try to understand whether the running job can negatively affects the pool stability so we have measured the condor_status response time while idle batch jobs get running as new slots become available. We have realized that response time raises in a linear way with the number of hosts/cores in other words the information about the running jobs does not affect che pool status. Furthermore, in a high load condition with 14K running jobs, a low standard deviation of the response time confirms the stability of the batch system. We have experimentally measured the HTCondor stability but at the same time we have had the opportunity to demonstrate how the collected data reflects the HTCondor architecture distributed design. The cluster and the batch aspect of the LRMS does not influence each other. This feature provide a huge contribution to the stability of the LRMS.
Fig. 2 : condor_status response time while idle batch jobs get running as new slots become available
3.3.2 HTCondor scalability
The entity which may vary from zero to a huge number is certainly the number of batch jobs submitted to the pool, because of that we have focused the scalability performance test on the batch jobs which can be managed by a single schedd. Fig. 3 show the results of this test  and highlights  the schedd scaling feature. We have measured such feature with the condor_q command response time as the number of batch jobs managed by the schedd increase or decrease over the time. 
Fig. 3: condor_q command returns information about all the submitted jobs in this test managed by only one schedd

Significant variations of idle jobs do not negatively affect the LRMS functionalities. Response time has a direct relationship  with the number of batch jobs managed by schedd and what is really important to highlight is the HTCondor schedd responsiveness actually nearly 120K idle batch jobs have not killed the schedd even better all of them have been correctly executed. You also have to take into account that there were a sustained number of running jobs about 16K.
4. HTCondor main features
HTCondor LRMS is a tool reach of useful features and during this experimental activity we have tried them taking into account those which could really have an important impact in the HTCondor deployment at ReCaS DataCenter.

Priority: resources  are allocated to users based upon a user’s priority. A lower numerical value for user priority means higher priority, so a user with priority 5 will get more resources than a user with priority 50.
Fairshare: it provides a fair access to the resources based upon user resources usage over the time and the user share assignment  of the  available resource.
Security: this is a broad issue which implement different security levels. It provides what if needed to authenticate and authorize users and daemons. It implements the encryption and integrity checking over the network
Ranking : used to choose a match from among all machines that satisfy the job’s requirements and are available to the user, after accounting for the user’s priority and the machine’s rank of the job. The rank expressions, simple or complex, define a numerical value that expresses preferences
Partitionable slots: allow to define a big slot that can be partitioned to accommodate actual needs by accepted jobs. It can address different CPU/memory requirements because it dynamically provides the need resources
Policy: it provides a way to control resource access and limiting the resource usage over the time
GRID interface: even if not well supported an HTCondor pool can be exploited through the GRID Computing

5. Conclusions
We have seen a LRMS which can easily defined stable, scalable and robust enough to be chosen as LRMS for ReCaS DataCenter. HTCondor can easily provides all the main functionalities in order that resources hosted at ReCaS DataCenter can be exploited through the batch job submission. Furthermore it is free of charge because it follows an open source philosophy and it's licensed under the Apache License 2.0. The LRMS is a scientific community driven software. It is manly developed by the University of Wisconsin with the contribution of other American Universities. Scientific community can influence the software development requesting new features and/or functionalities and at the same time provides best effort support through two mailing lists and tutorials.
